# GENETARED BY NNDCT, DO NOT EDIT!

import torch
from torch import tensor
import pytorch_nndct as py_nndct

class QuantLeNet5(py_nndct.nn.NndctQuantModel):
    def __init__(self):
        super(QuantLeNet5, self).__init__()
        self.module_0 = py_nndct.nn.Input() #QuantLeNet5::input_0(QuantLeNet5::nndct_input_0)
        self.module_1 = py_nndct.nn.Module('aten::fused_moving_avg_obs_fake_quant') #QuantLeNet5::QuantLeNet5/FusedMovingAvgObsFakeQuantize[activation_post_process]/ret.3(QuantLeNet5::aten_fused_moving_avg_obs_fake_quant_1)
        self.module_2 = py_nndct.nn.Module('aten::fused_moving_avg_obs_fake_quant') #QuantLeNet5::QuantLeNet5/Conv2d[conv1]/FusedMovingAvgObsFakeQuantize[weight_fake_quant]/866(QuantLeNet5::aten_fused_moving_avg_obs_fake_quant_2)
        self.module_3 = py_nndct.nn.Module('aten::_convolution') #QuantLeNet5::QuantLeNet5/Conv2d[conv1]/ret.5(QuantLeNet5::aten__convolution_3)
        self.module_4 = py_nndct.nn.Module('aten::fused_moving_avg_obs_fake_quant') #QuantLeNet5::QuantLeNet5/FusedMovingAvgObsFakeQuantize[activation_post_process]/ret.7(QuantLeNet5::aten_fused_moving_avg_obs_fake_quant_4)
        self.module_5 = py_nndct.nn.ReLU(inplace=False) #QuantLeNet5::QuantLeNet5/ReLU[relu]/ret.9(QuantLeNet5::nndct_relu_5)
        self.module_6 = py_nndct.nn.AvgPool2d(kernel_size=[2, 2], stride=[2, 2], padding=[0, 0], ceil_mode=False, count_include_pad=True) #QuantLeNet5::QuantLeNet5/AvgPool2d[avg_pool]/910(QuantLeNet5::nndct_avgpool_6)
        self.module_7 = py_nndct.nn.Module('aten::fused_moving_avg_obs_fake_quant') #QuantLeNet5::QuantLeNet5/Conv2d[conv2]/FusedMovingAvgObsFakeQuantize[weight_fake_quant]/918(QuantLeNet5::aten_fused_moving_avg_obs_fake_quant_7)
        self.module_8 = py_nndct.nn.Module('aten::_convolution') #QuantLeNet5::QuantLeNet5/Conv2d[conv2]/ret.11(QuantLeNet5::aten__convolution_8)
        self.module_9 = py_nndct.nn.Module('aten::fused_moving_avg_obs_fake_quant') #QuantLeNet5::QuantLeNet5/FusedMovingAvgObsFakeQuantize[activation_post_process]/ret.13(QuantLeNet5::aten_fused_moving_avg_obs_fake_quant_9)
        self.module_10 = py_nndct.nn.ReLU(inplace=False) #QuantLeNet5::QuantLeNet5/ReLU[relu]/ret.15(QuantLeNet5::nndct_relu_10)
        self.module_11 = py_nndct.nn.AvgPool2d(kernel_size=[2, 2], stride=[2, 2], padding=[0, 0], ceil_mode=False, count_include_pad=True) #QuantLeNet5::QuantLeNet5/AvgPool2d[avg_pool]/962(QuantLeNet5::nndct_avgpool_11)
        self.module_12 = py_nndct.nn.Module('nndct_shape') #QuantLeNet5::QuantLeNet5/965(QuantLeNet5::nndct_shape_12)
        self.module_13 = py_nndct.nn.Module('nndct_reshape') #QuantLeNet5::QuantLeNet5/ret.19(QuantLeNet5::nndct_reshape_13)
        self.module_14 = py_nndct.nn.Module('aten::fused_moving_avg_obs_fake_quant') #QuantLeNet5::QuantLeNet5/Linear[fc1]/FusedMovingAvgObsFakeQuantize[weight_fake_quant]/979(QuantLeNet5::aten_fused_moving_avg_obs_fake_quant_14)
        self.module_15 = py_nndct.nn.Module('aten::linear') #QuantLeNet5::QuantLeNet5/Linear[fc1]/ret.21(QuantLeNet5::aten_linear_15)
        self.module_16 = py_nndct.nn.Module('aten::fused_moving_avg_obs_fake_quant') #QuantLeNet5::QuantLeNet5/FusedMovingAvgObsFakeQuantize[activation_post_process]/ret.23(QuantLeNet5::aten_fused_moving_avg_obs_fake_quant_16)
        self.module_17 = py_nndct.nn.ReLU(inplace=False) #QuantLeNet5::QuantLeNet5/ReLU[relu]/ret.25(QuantLeNet5::nndct_relu_17)
        self.module_18 = py_nndct.nn.Module('aten::fused_moving_avg_obs_fake_quant') #QuantLeNet5::QuantLeNet5/Linear[fc2]/FusedMovingAvgObsFakeQuantize[weight_fake_quant]/998(QuantLeNet5::aten_fused_moving_avg_obs_fake_quant_18)
        self.module_19 = py_nndct.nn.Module('aten::linear') #QuantLeNet5::QuantLeNet5/Linear[fc2]/ret.27(QuantLeNet5::aten_linear_19)
        self.module_20 = py_nndct.nn.Module('aten::fused_moving_avg_obs_fake_quant') #QuantLeNet5::QuantLeNet5/FusedMovingAvgObsFakeQuantize[activation_post_process]/ret.29(QuantLeNet5::aten_fused_moving_avg_obs_fake_quant_20)
        self.module_21 = py_nndct.nn.ReLU(inplace=False) #QuantLeNet5::QuantLeNet5/ReLU[relu]/ret.31(QuantLeNet5::nndct_relu_21)
        self.module_22 = py_nndct.nn.Module('aten::fused_moving_avg_obs_fake_quant') #QuantLeNet5::QuantLeNet5/Linear[fc3]/FusedMovingAvgObsFakeQuantize[weight_fake_quant]/1017(QuantLeNet5::aten_fused_moving_avg_obs_fake_quant_22)
        self.module_23 = py_nndct.nn.Module('aten::linear') #QuantLeNet5::QuantLeNet5/Linear[fc3]/ret.33(QuantLeNet5::aten_linear_23)
        self.module_24 = py_nndct.nn.Module('aten::fused_moving_avg_obs_fake_quant') #QuantLeNet5::QuantLeNet5/FusedMovingAvgObsFakeQuantize[activation_post_process]/ret(QuantLeNet5::aten_fused_moving_avg_obs_fake_quant_24)
        self.quant_activation_post_process_observer_enabled = torch.nn.parameter.Parameter(torch.Tensor(1,))
        self.quant_activation_post_process_fake_quant_enabled = torch.nn.parameter.Parameter(torch.Tensor(1,))
        self.quant_activation_post_process_activation_post_process_min_val = torch.nn.parameter.Parameter(torch.Tensor())
        self.quant_activation_post_process_activation_post_process_max_val = torch.nn.parameter.Parameter(torch.Tensor())
        self.quant_activation_post_process_scale = torch.nn.parameter.Parameter(torch.Tensor(1,))
        self.quant_activation_post_process_zero_point = torch.nn.parameter.Parameter(torch.Tensor(1,))
        self.conv1_weight = torch.nn.parameter.Parameter(torch.Tensor(6, 1, 5, 5))
        self.conv1_weight_fake_quant_observer_enabled = torch.nn.parameter.Parameter(torch.Tensor(1,))
        self.conv1_weight_fake_quant_fake_quant_enabled = torch.nn.parameter.Parameter(torch.Tensor(1,))
        self.conv1_weight_fake_quant_activation_post_process_min_val = torch.nn.parameter.Parameter(torch.Tensor(6,))
        self.conv1_weight_fake_quant_activation_post_process_max_val = torch.nn.parameter.Parameter(torch.Tensor(6,))
        self.conv1_weight_fake_quant_scale = torch.nn.parameter.Parameter(torch.Tensor(6,))
        self.conv1_weight_fake_quant_zero_point = torch.nn.parameter.Parameter(torch.Tensor(6,))
        self.conv1_bias = torch.nn.parameter.Parameter(torch.Tensor(6,))
        self.conv1_activation_post_process_observer_enabled = torch.nn.parameter.Parameter(torch.Tensor(1,))
        self.conv1_activation_post_process_fake_quant_enabled = torch.nn.parameter.Parameter(torch.Tensor(1,))
        self.conv1_activation_post_process_activation_post_process_min_val = torch.nn.parameter.Parameter(torch.Tensor())
        self.conv1_activation_post_process_activation_post_process_max_val = torch.nn.parameter.Parameter(torch.Tensor())
        self.conv1_activation_post_process_scale = torch.nn.parameter.Parameter(torch.Tensor(1,))
        self.conv1_activation_post_process_zero_point = torch.nn.parameter.Parameter(torch.Tensor(1,))
        self.conv2_weight = torch.nn.parameter.Parameter(torch.Tensor(16, 6, 5, 5))
        self.conv2_weight_fake_quant_observer_enabled = torch.nn.parameter.Parameter(torch.Tensor(1,))
        self.conv2_weight_fake_quant_fake_quant_enabled = torch.nn.parameter.Parameter(torch.Tensor(1,))
        self.conv2_weight_fake_quant_activation_post_process_min_val = torch.nn.parameter.Parameter(torch.Tensor(16,))
        self.conv2_weight_fake_quant_activation_post_process_max_val = torch.nn.parameter.Parameter(torch.Tensor(16,))
        self.conv2_weight_fake_quant_scale = torch.nn.parameter.Parameter(torch.Tensor(16,))
        self.conv2_weight_fake_quant_zero_point = torch.nn.parameter.Parameter(torch.Tensor(16,))
        self.conv2_bias = torch.nn.parameter.Parameter(torch.Tensor(16,))
        self.conv2_activation_post_process_observer_enabled = torch.nn.parameter.Parameter(torch.Tensor(1,))
        self.conv2_activation_post_process_fake_quant_enabled = torch.nn.parameter.Parameter(torch.Tensor(1,))
        self.conv2_activation_post_process_activation_post_process_min_val = torch.nn.parameter.Parameter(torch.Tensor())
        self.conv2_activation_post_process_activation_post_process_max_val = torch.nn.parameter.Parameter(torch.Tensor())
        self.conv2_activation_post_process_scale = torch.nn.parameter.Parameter(torch.Tensor(1,))
        self.conv2_activation_post_process_zero_point = torch.nn.parameter.Parameter(torch.Tensor(1,))
        self.fc1_weight = torch.nn.parameter.Parameter(torch.Tensor(120, 256))
        self.fc1_weight_fake_quant_observer_enabled = torch.nn.parameter.Parameter(torch.Tensor(1,))
        self.fc1_weight_fake_quant_fake_quant_enabled = torch.nn.parameter.Parameter(torch.Tensor(1,))
        self.fc1_weight_fake_quant_activation_post_process_min_val = torch.nn.parameter.Parameter(torch.Tensor(120,))
        self.fc1_weight_fake_quant_activation_post_process_max_val = torch.nn.parameter.Parameter(torch.Tensor(120,))
        self.fc1_weight_fake_quant_scale = torch.nn.parameter.Parameter(torch.Tensor(120,))
        self.fc1_weight_fake_quant_zero_point = torch.nn.parameter.Parameter(torch.Tensor(120,))
        self.fc1_bias = torch.nn.parameter.Parameter(torch.Tensor(120,))
        self.fc1_activation_post_process_observer_enabled = torch.nn.parameter.Parameter(torch.Tensor(1,))
        self.fc1_activation_post_process_fake_quant_enabled = torch.nn.parameter.Parameter(torch.Tensor(1,))
        self.fc1_activation_post_process_activation_post_process_min_val = torch.nn.parameter.Parameter(torch.Tensor())
        self.fc1_activation_post_process_activation_post_process_max_val = torch.nn.parameter.Parameter(torch.Tensor())
        self.fc1_activation_post_process_scale = torch.nn.parameter.Parameter(torch.Tensor(1,))
        self.fc1_activation_post_process_zero_point = torch.nn.parameter.Parameter(torch.Tensor(1,))
        self.fc2_weight = torch.nn.parameter.Parameter(torch.Tensor(84, 120))
        self.fc2_weight_fake_quant_observer_enabled = torch.nn.parameter.Parameter(torch.Tensor(1,))
        self.fc2_weight_fake_quant_fake_quant_enabled = torch.nn.parameter.Parameter(torch.Tensor(1,))
        self.fc2_weight_fake_quant_activation_post_process_min_val = torch.nn.parameter.Parameter(torch.Tensor(84,))
        self.fc2_weight_fake_quant_activation_post_process_max_val = torch.nn.parameter.Parameter(torch.Tensor(84,))
        self.fc2_weight_fake_quant_scale = torch.nn.parameter.Parameter(torch.Tensor(84,))
        self.fc2_weight_fake_quant_zero_point = torch.nn.parameter.Parameter(torch.Tensor(84,))
        self.fc2_bias = torch.nn.parameter.Parameter(torch.Tensor(84,))
        self.fc2_activation_post_process_observer_enabled = torch.nn.parameter.Parameter(torch.Tensor(1,))
        self.fc2_activation_post_process_fake_quant_enabled = torch.nn.parameter.Parameter(torch.Tensor(1,))
        self.fc2_activation_post_process_activation_post_process_min_val = torch.nn.parameter.Parameter(torch.Tensor())
        self.fc2_activation_post_process_activation_post_process_max_val = torch.nn.parameter.Parameter(torch.Tensor())
        self.fc2_activation_post_process_scale = torch.nn.parameter.Parameter(torch.Tensor(1,))
        self.fc2_activation_post_process_zero_point = torch.nn.parameter.Parameter(torch.Tensor(1,))
        self.fc3_weight = torch.nn.parameter.Parameter(torch.Tensor(10, 84))
        self.fc3_weight_fake_quant_observer_enabled = torch.nn.parameter.Parameter(torch.Tensor(1,))
        self.fc3_weight_fake_quant_fake_quant_enabled = torch.nn.parameter.Parameter(torch.Tensor(1,))
        self.fc3_weight_fake_quant_activation_post_process_min_val = torch.nn.parameter.Parameter(torch.Tensor(10,))
        self.fc3_weight_fake_quant_activation_post_process_max_val = torch.nn.parameter.Parameter(torch.Tensor(10,))
        self.fc3_weight_fake_quant_scale = torch.nn.parameter.Parameter(torch.Tensor(10,))
        self.fc3_weight_fake_quant_zero_point = torch.nn.parameter.Parameter(torch.Tensor(10,))
        self.fc3_bias = torch.nn.parameter.Parameter(torch.Tensor(10,))
        self.fc3_activation_post_process_observer_enabled = torch.nn.parameter.Parameter(torch.Tensor(1,))
        self.fc3_activation_post_process_fake_quant_enabled = torch.nn.parameter.Parameter(torch.Tensor(1,))
        self.fc3_activation_post_process_activation_post_process_min_val = torch.nn.parameter.Parameter(torch.Tensor())
        self.fc3_activation_post_process_activation_post_process_max_val = torch.nn.parameter.Parameter(torch.Tensor())
        self.fc3_activation_post_process_scale = torch.nn.parameter.Parameter(torch.Tensor(1,))
        self.fc3_activation_post_process_zero_point = torch.nn.parameter.Parameter(torch.Tensor(1,))

    @py_nndct.nn.forward_processor
    def forward(self, *args):
        output_module_0 = self.module_0(input=args[0])
        output_module_0 = self.module_1({'self': output_module_0,'observer_on': self.quant_activation_post_process_observer_enabled,'fake_quant_on': self.quant_activation_post_process_fake_quant_enabled,'running_min': self.quant_activation_post_process_activation_post_process_min_val,'running_max': self.quant_activation_post_process_activation_post_process_max_val,'scale': self.quant_activation_post_process_scale,'zero_point': self.quant_activation_post_process_zero_point,'averaging_const': 0.01,'quant_min': 0,'quant_max': 127,'ch_axis': -1,'per_row_fake_quant': False,'symmetric_quant': False})
        output_module_2 = self.module_2({'self': self.conv1_weight,'observer_on': self.conv1_weight_fake_quant_observer_enabled,'fake_quant_on': self.conv1_weight_fake_quant_fake_quant_enabled,'running_min': self.conv1_weight_fake_quant_activation_post_process_min_val,'running_max': self.conv1_weight_fake_quant_activation_post_process_max_val,'scale': self.conv1_weight_fake_quant_scale,'zero_point': self.conv1_weight_fake_quant_zero_point,'averaging_const': 0.01,'quant_min': -128,'quant_max': 127,'ch_axis': 0,'per_row_fake_quant': True,'symmetric_quant': True})
        output_module_0 = self.module_3({'input': output_module_0,'weight': output_module_2,'bias': self.conv1_bias,'stride': [1,1],'padding': [0,0],'dilation': [1,1],'transposed': False,'output_padding': [0,0],'groups': 1,'benchmark': False,'deterministic': False,'cudnn_enabled': True,'allow_tf32': True})
        output_module_0 = self.module_4({'self': output_module_0,'observer_on': self.conv1_activation_post_process_observer_enabled,'fake_quant_on': self.conv1_activation_post_process_fake_quant_enabled,'running_min': self.conv1_activation_post_process_activation_post_process_min_val,'running_max': self.conv1_activation_post_process_activation_post_process_max_val,'scale': self.conv1_activation_post_process_scale,'zero_point': self.conv1_activation_post_process_zero_point,'averaging_const': 0.01,'quant_min': 0,'quant_max': 127,'ch_axis': -1,'per_row_fake_quant': False,'symmetric_quant': False})
        output_module_0 = self.module_5(output_module_0)
        output_module_0 = self.module_6(output_module_0)
        output_module_7 = self.module_7({'self': self.conv2_weight,'observer_on': self.conv2_weight_fake_quant_observer_enabled,'fake_quant_on': self.conv2_weight_fake_quant_fake_quant_enabled,'running_min': self.conv2_weight_fake_quant_activation_post_process_min_val,'running_max': self.conv2_weight_fake_quant_activation_post_process_max_val,'scale': self.conv2_weight_fake_quant_scale,'zero_point': self.conv2_weight_fake_quant_zero_point,'averaging_const': 0.01,'quant_min': -128,'quant_max': 127,'ch_axis': 0,'per_row_fake_quant': True,'symmetric_quant': True})
        output_module_0 = self.module_8({'input': output_module_0,'weight': output_module_7,'bias': self.conv2_bias,'stride': [1,1],'padding': [0,0],'dilation': [1,1],'transposed': False,'output_padding': [0,0],'groups': 1,'benchmark': False,'deterministic': False,'cudnn_enabled': True,'allow_tf32': True})
        output_module_0 = self.module_9({'self': output_module_0,'observer_on': self.conv2_activation_post_process_observer_enabled,'fake_quant_on': self.conv2_activation_post_process_fake_quant_enabled,'running_min': self.conv2_activation_post_process_activation_post_process_min_val,'running_max': self.conv2_activation_post_process_activation_post_process_max_val,'scale': self.conv2_activation_post_process_scale,'zero_point': self.conv2_activation_post_process_zero_point,'averaging_const': 0.01,'quant_min': 0,'quant_max': 127,'ch_axis': -1,'per_row_fake_quant': False,'symmetric_quant': False})
        output_module_0 = self.module_10(output_module_0)
        output_module_0 = self.module_11(output_module_0)
        output_module_12 = self.module_12(input=output_module_0, dim=0)
        output_module_13 = self.module_13(input=output_module_0, shape=[output_module_12,-1])
        output_module_14 = self.module_14({'self': self.fc1_weight,'observer_on': self.fc1_weight_fake_quant_observer_enabled,'fake_quant_on': self.fc1_weight_fake_quant_fake_quant_enabled,'running_min': self.fc1_weight_fake_quant_activation_post_process_min_val,'running_max': self.fc1_weight_fake_quant_activation_post_process_max_val,'scale': self.fc1_weight_fake_quant_scale,'zero_point': self.fc1_weight_fake_quant_zero_point,'averaging_const': 0.01,'quant_min': -128,'quant_max': 127,'ch_axis': 0,'per_row_fake_quant': True,'symmetric_quant': True})
        output_module_13 = self.module_15({'input': output_module_13,'weight': output_module_14,'bias': self.fc1_bias})
        output_module_13 = self.module_16({'self': output_module_13,'observer_on': self.fc1_activation_post_process_observer_enabled,'fake_quant_on': self.fc1_activation_post_process_fake_quant_enabled,'running_min': self.fc1_activation_post_process_activation_post_process_min_val,'running_max': self.fc1_activation_post_process_activation_post_process_max_val,'scale': self.fc1_activation_post_process_scale,'zero_point': self.fc1_activation_post_process_zero_point,'averaging_const': 0.01,'quant_min': 0,'quant_max': 127,'ch_axis': -1,'per_row_fake_quant': False,'symmetric_quant': False})
        output_module_13 = self.module_17(output_module_13)
        output_module_18 = self.module_18({'self': self.fc2_weight,'observer_on': self.fc2_weight_fake_quant_observer_enabled,'fake_quant_on': self.fc2_weight_fake_quant_fake_quant_enabled,'running_min': self.fc2_weight_fake_quant_activation_post_process_min_val,'running_max': self.fc2_weight_fake_quant_activation_post_process_max_val,'scale': self.fc2_weight_fake_quant_scale,'zero_point': self.fc2_weight_fake_quant_zero_point,'averaging_const': 0.01,'quant_min': -128,'quant_max': 127,'ch_axis': 0,'per_row_fake_quant': True,'symmetric_quant': True})
        output_module_13 = self.module_19({'input': output_module_13,'weight': output_module_18,'bias': self.fc2_bias})
        output_module_13 = self.module_20({'self': output_module_13,'observer_on': self.fc2_activation_post_process_observer_enabled,'fake_quant_on': self.fc2_activation_post_process_fake_quant_enabled,'running_min': self.fc2_activation_post_process_activation_post_process_min_val,'running_max': self.fc2_activation_post_process_activation_post_process_max_val,'scale': self.fc2_activation_post_process_scale,'zero_point': self.fc2_activation_post_process_zero_point,'averaging_const': 0.01,'quant_min': 0,'quant_max': 127,'ch_axis': -1,'per_row_fake_quant': False,'symmetric_quant': False})
        output_module_13 = self.module_21(output_module_13)
        output_module_22 = self.module_22({'self': self.fc3_weight,'observer_on': self.fc3_weight_fake_quant_observer_enabled,'fake_quant_on': self.fc3_weight_fake_quant_fake_quant_enabled,'running_min': self.fc3_weight_fake_quant_activation_post_process_min_val,'running_max': self.fc3_weight_fake_quant_activation_post_process_max_val,'scale': self.fc3_weight_fake_quant_scale,'zero_point': self.fc3_weight_fake_quant_zero_point,'averaging_const': 0.01,'quant_min': -128,'quant_max': 127,'ch_axis': 0,'per_row_fake_quant': True,'symmetric_quant': True})
        output_module_13 = self.module_23({'input': output_module_13,'weight': output_module_22,'bias': self.fc3_bias})
        output_module_13 = self.module_24({'self': output_module_13,'observer_on': self.fc3_activation_post_process_observer_enabled,'fake_quant_on': self.fc3_activation_post_process_fake_quant_enabled,'running_min': self.fc3_activation_post_process_activation_post_process_min_val,'running_max': self.fc3_activation_post_process_activation_post_process_max_val,'scale': self.fc3_activation_post_process_scale,'zero_point': self.fc3_activation_post_process_zero_point,'averaging_const': 0.01,'quant_min': 0,'quant_max': 127,'ch_axis': -1,'per_row_fake_quant': False,'symmetric_quant': False})
        return output_module_13
